{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dependencies; setup logger\n",
    "import torch, torchvision\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import DatasetMapper   # the default mapper\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader\n",
    "from fvcore.transforms.transform import NoOpTransform\n",
    "\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "import detectron2.utils.comm as comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom dataset; register it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weed_dicts(img_dir, file_list):\n",
    "    json_file = os.path.join(img_dir, \"_weed_labels.json\")\n",
    "    with open(json_file) as f:\n",
    "        img_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(img_anns.values()):\n",
    "        if v[\"filename\"] in file_list:  # train or test data\n",
    "            record = {}\n",
    "            filename = os.path.join(img_dir, v[\"filename\"])\n",
    "            # print(filename)\n",
    "            height, width = cv2.imread(filename).shape[:2]\n",
    "\n",
    "            record[\"file_name\"] = filename\n",
    "            record[\"image_id\"] = idx\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "\n",
    "            annos = v[\"regions\"]   # List of object attributes\n",
    "            objs = []\n",
    "            for anno in annos:\n",
    "                if anno[\"region_attributes\"][\"label\"] == \"weed\":\n",
    "                    sa = anno[\"shape_attributes\"]\n",
    "                    obj = {\n",
    "                        \"bbox\": [sa['x'], sa['y'], sa['width'], sa['height']],\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS, # or XYXY_ABS\n",
    "                        \"category_id\": 0 if anno[\"region_attributes\"][\"label\"] == \"weed\" else 1\n",
    "                    }\n",
    "                    objs.append(obj)\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and register datasets\n",
    "# img_dir = '/home/mschoder/data/allweeds_600x400/'\n",
    "img_dir = '/home/mschoder/data/allweeds_1200x800/'\n",
    "\n",
    "json_file = os.path.join(img_dir, \"_weed_labels.json\")\n",
    "print(json_file)\n",
    "with open(json_file) as f:\n",
    "    img_anns = json.load(f)\n",
    "\n",
    "file_list = sorted([k for k in img_anns.keys()])\n",
    "print(\"Number of images: \", len(file_list))\n",
    "np.random.seed(31)\n",
    "np.random.shuffle(file_list)\n",
    "val_pct = 0.3  \n",
    "file_lists = {\n",
    "    \"val\": file_list[:int(val_pct * len(file_list))],\n",
    "    \"train\": file_list[int(val_pct * len(file_list)):]\n",
    "}\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(\"weeds_\" + d, lambda d=d: get_weed_dicts(img_dir, file_lists[d]))\n",
    "    MetadataCatalog.get(\"weeds_\" + d).set(thing_classes=[\"weed\"])\n",
    "weeds_metadata = MetadataCatalog.get(\"weeds_train\")\n",
    "\n",
    "# Build full dict\n",
    "print('Creating datasets...')\n",
    "# print(file_lists)\n",
    "dataset_dicts = get_weed_dicts(img_dir, file_lists['train'])\n",
    "print('Datasets created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a custom trainer which inherits from default\n",
    "- Get custom eval metrics while training\n",
    "- Apply desired augmentations / transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mapper_train(dataset_dict):\n",
    "    # Implement a mapper, similar to the default DatasetMapper, but with your own customizations\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"RGB\")\n",
    "    transform_list = []\n",
    "    # Rotate 90 degrees to landscape if image is in portrait format\n",
    "    height, width, _ = image.shape\n",
    "    if height > width:\n",
    "        transform_list.append(T.RotationTransform)\n",
    "    transform_list = [T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "                      NoOpTransform(),\n",
    "                      ]\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict\n",
    "\n",
    "def custom_mapper_test(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict) \n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"RGB\")\n",
    "    transform_list = []\n",
    "    height, width, _ = image.shape\n",
    "    if height > width:\n",
    "        transform_list.append(T.RotationTransform)\n",
    "    transform_list = [NoOpTransform(),\n",
    "                      ]\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict\n",
    "\n",
    "## Define hook for validation loss\n",
    "class ValidationLoss(HookBase):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg.clone()\n",
    "        self.cfg.DATASETS.TRAIN = cfg.DATASETS.TEST\n",
    "        self._loader = iter(build_detection_train_loader(self.cfg, mapper=custom_mapper_test)) \n",
    "        \n",
    "    def after_step(self):\n",
    "        data = next(self._loader)\n",
    "        with torch.no_grad():\n",
    "            loss_dict = self.trainer.model(data)\n",
    "            \n",
    "            losses = sum(loss_dict.values())\n",
    "            assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "            loss_dict_reduced = {\"val_\" + k: v.item() for k, v in \n",
    "                                 comm.reduce_dict(loss_dict).items()}\n",
    "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "            if comm.is_main_process():\n",
    "                self.trainer.storage.put_scalars(total_val_loss=losses_reduced, \n",
    "                                                 **loss_dict_reduced)\n",
    "\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "            output_folder = \"coco_eval\"\n",
    "\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=custom_mapper_train)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build_detection_test_loader(cfg, cfg.DATASETS.TEST[0], mapper=custom_mapper_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING CONFIG\n",
    "\n",
    "pretrained_models = [\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\",\n",
    "                     \"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\",\n",
    "                     \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"]\n",
    "\n",
    "# color_tfs = [NoOpTransform(), HSV_EQ_Transform(), \n",
    "#              HLS_EQ_Transform(), NDI_CIVE_ExG_Transform()]\n",
    "color_tfs = [NoOpTransform()]\n",
    "\n",
    "m_id = 2\n",
    "tf_id = 0\n",
    "\n",
    "# Select base model & initialized weights\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(pretrained_models[m_id]))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(pretrained_models[m_id]) \n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"weeds_train\",)\n",
    "cfg.DATASETS.TEST = (\"weeds_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # TODO - tweak\n",
    "cfg.SOLVER.MAX_ITER = 1000   # Adjusted based on validation mAP (500-1500)\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only one class (weed), other class is background\n",
    "\n",
    "cfg.TEST.EVAL_PERIOD = 100  # every 5 epochs\n",
    "\n",
    "cfg.OUTPUT_DIR = '/home/mschoder/experiment_outputs/' + \\\n",
    "                 '1200x800_' + \\\n",
    "                 'model_' + str(pretrained_models[m_id]) + '_' + \\\n",
    "                 'lr_' + str(cfg.SOLVER.BASE_LR) + '_' + \\\n",
    "                 'iters_' + str(750) + '_' + \\\n",
    "                 'tf_' + str(color_tfs[tf_id]) + '/'\n",
    "\n",
    "# Set everything up to train, register hooks\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CocoTrainer(cfg)\n",
    "# val_loss = ValidationLoss(cfg)\n",
    "# trainer.register_hooks([val_loss])\n",
    "# swap the order of PeriodicWriter and ValidationLoss\n",
    "# trainer._hooks = trainer._hooks[:-2] + trainer._hooks[-2:][::-1]\n",
    "\n",
    "trainer.resume_or_load(resume=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "print('Starting training for config: ', cfg.OUTPUT_DIR)\n",
    "trainer.train()\n",
    "print('Training succeeded for config: ', cfg.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train-Validation Loss curve\n",
    "\n",
    "# experiment_folder = './output/model_iter4000_lr0005_wf1_date2020_03_20__05_16_45'\n",
    "# experiment_folder = '/home/mschoder/sugarcane-weed-classification/models/data/output'\n",
    "# experiment_folder = '/home/mschoder/sugarcane-weed-classification/models/home/mschoder/experiemnt_outputs/model_0_lr_0.00025_iters_1500_tf_NDI_CIVE_ExG_Transform()'\n",
    "experiment_folder = cfg.OUTPUT_DIR\n",
    "\n",
    "def load_json_arr(json_path):\n",
    "    lines = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
    "\n",
    "iters = [x['iteration'] for x in experiment_metrics if 'total_val_loss' in x.keys()]\n",
    "total_losses = [x['total_loss'] for x in experiment_metrics if 'total_val_loss' in x.keys()]\n",
    "val_losses = [x['total_val_loss'] for x in experiment_metrics if 'total_val_loss' in x]\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10,7))\n",
    "# plt.plot(iters, total_losses)\n",
    "# plt.plot(iters, val_losses)\n",
    "# plt.legend(['total_loss', 'validation_loss'], loc='upper right')\n",
    "# plt.title('Train-Validation Loss')\n",
    "# plt.xlabel('Training Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get mAP validation metrics & others\n",
    "val_loss_metrics = [x for x in experiment_metrics if 'total_val_loss' in x]\n",
    "# print(len(val_loss_metrics))\n",
    "\n",
    "bbox_loss_metrics = [x for x in experiment_metrics if 'bbox/AP50' in x]\n",
    "# print(len(bbox_loss_metrics))\n",
    "\n",
    "class_acc_metrics = [x for x in experiment_metrics if 'fast_rcnn/cls_accuracy' in x]\n",
    "# print(len(class_acc_metrics))\n",
    "\n",
    "false_neg_metrics = [x for x in experiment_metrics if 'fast_rcnn/false_negative' in x]\n",
    "# print(len(false_neg_metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,7))\n",
    "# plt.plot([x['iteration'] for x in class_acc_metrics],\n",
    "#         [x['fast_rcnn/cls_accuracy'] for x in class_acc_metrics])\n",
    "# plt.plot([x['iteration'] for x in false_neg_metrics],\n",
    "#         [x['fast_rcnn/false_negative'] for x in false_neg_metrics])\n",
    "# plt.legend(['class accuracy', 'false_negative'], loc='upper right')\n",
    "# plt.title('Accuracy and False Negative Rate (Validation)')\n",
    "# plt.xlabel('Training Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ap50 = [x['bbox/AP50'] for x in experiment_metrics if 'bbox/AP50' in x]\n",
    "val_ap_iters = [x['iteration'] for x in experiment_metrics if 'bbox/AP50' in x]\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(val_ap_iters, val_ap50)\n",
    "plt.title('Average Precision at IOU=0.50 (Validation)')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Average Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to trained model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\n",
    "cfg.INSTANCES_CONFIDENCE_THRESH = 0.7\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(12, 15), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .3, wspace=.3)\n",
    "fig.tight_layout()\n",
    "axs = axs.ravel()\n",
    "\n",
    "val_dict = get_weed_dicts(img_dir, file_lists['val'])\n",
    "for i,d in enumerate(random.sample(val_dict, 2)):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=weeds_metadata, \n",
    "                   scale=1)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    axs[i].imshow(out.get_image())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processs Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video_thru_model(input, output, predictor, out_size=(1200,800), coverage_threshold = None):\n",
    "\n",
    "    cap = cv2.VideoCapture(input)\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') # Codec format\n",
    "    out_writer = cv2.VideoWriter(output, fourcc, 20.0, out_size)  \n",
    "\n",
    "    recent_frames = [0] * 15  # moving average of last N frames\n",
    "\n",
    "    framecount = 0\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        moving_avg = 0\n",
    "        if ret==True:\n",
    "            framecount += 1\n",
    "            if framecount % 100 == 0:\n",
    "                print(framecount, \" Frames Processed\")\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = predictor(frame)\n",
    "            # print(outputs)\n",
    "\n",
    "            # Calculate weed coverage\n",
    "            if coverage_threshold is not None:\n",
    "                total_area = frame.shape[0] * frame.shape[1]\n",
    "                weed_area = 0\n",
    "                for inst in range(len(outputs['instances'])):\n",
    "                    coords = outputs['instances'][inst].get('pred_boxes').tensor.tolist()[0]\n",
    "                    box_area = (coords[2] - coords[0]) * (coords[3] - coords[1])\n",
    "                    weed_area += box_area\n",
    "                recent_frames = recent_frames[1:]\n",
    "                recent_frames.append(weed_area/total_area)\n",
    "                moving_avg = sum(recent_frames) / len(recent_frames)\n",
    "\n",
    "            # Draw bboxes\n",
    "            v = Visualizer(frame[:, :, ::-1],\n",
    "                    metadata=weeds_metadata, \n",
    "                    scale=1, \n",
    "                    instance_mode=ColorMode.IMAGE\n",
    "            )\n",
    "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            outframe = out.get_image()[:, :, ::-1]\n",
    "            outframe = np.array(outframe)\n",
    "\n",
    "            # Draw activation indicator if above threshold\n",
    "            if moving_avg > coverage_threshold:\n",
    "                outframe = cv2.rectangle(outframe,(10,10),(frame.shape[1]-10, frame.shape[0]-10),(0,128,255),12)\n",
    "            outframe = cv2.putText(outframe, str(round(moving_avg,2)), (285, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255,255,255), 2)\n",
    "\n",
    "            # write the output frame\n",
    "            out_writer.write(outframe)\n",
    "\n",
    "            # cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Uncomment below to run video processing workflow\n",
    "\n",
    "input_vid = '/home/mschoder/data/raw_video/test_vid_1200x800.avi'\n",
    "output_vid = '/home/mschoder/data/raw_video/processed_1200x800_hsv_750.avi'\n",
    "run_video_thru_model(input_vid, output_vid, predictor, coverage_threshold=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
