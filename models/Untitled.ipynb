{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "# import utils\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SugarcaneWeedsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_ids = list(sorted(os.listdir(os.path.join(root, \"jpg_images\"))))\n",
    "        self.img_dir = pathlib.Path(data_dir) / \"jpg_images\"\n",
    "        with open(os.path.join(data_dir, \"labels.json\")) as f:\n",
    "            self.annot = json.load(f)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        annotations = self.annot[img_id]\n",
    "        print(annotations)\n",
    "        \n",
    "        img = Image.open(self.img_dir/img_id).convert('RGB')\n",
    "        img = T.ToTensor()(img)\n",
    "        \n",
    "        box_dict = annotations['regions']\n",
    "        obj_classes = [x['region_attributes']['label'] for x in box_dict]\n",
    "        num_classes = len(set(obj_classes))\n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_classes)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            bd   = box_dict[i]['shape_attributes']\n",
    "            xmin = bd['x']\n",
    "            xmax = bd['x'] + bd['width']\n",
    "            ymin = bd['y']\n",
    "            ymax = bd['y'] + bd['height']\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.tensor(boxes, dtype=torch.int64)\n",
    "        \n",
    "        # instance labels\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        # class labels - TODO\n",
    "        # or just use weed labels\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['img_id'] = torch.tensor([idx])\n",
    "        \n",
    "        return img, target, img_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pretrained model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out if too many boxes for a single instance\n",
    "keep = torchvision.ops.nms(boxes, scores, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SugarcaneWeedsDataset at 0x13cdfba90>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Users/mschoder/weeding_project/box_labeled_data'\n",
    "test = SugarcaneWeedsDataset(data_dir)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'i3092.jpg', 'size': 6182682, 'regions': [{'shape_attributes': {'name': 'rect', 'x': 887, 'y': 3249, 'width': 1577, 'height': 1352}, 'region_attributes': {'label': 'weed'}}, {'shape_attributes': {'name': 'rect', 'x': 7, 'y': 0, 'width': 3058, 'height': 1522}, 'region_attributes': {'label': 'weed'}}, {'shape_attributes': {'name': 'rect', 'x': 608, 'y': 1563, 'width': 2444, 'height': 1673}, 'region_attributes': {'label': 'weed'}}, {'shape_attributes': {'name': 'rect', 'x': 2499, 'y': 3639, 'width': 560, 'height': 963}, 'region_attributes': {'label': 'weed'}}, {'shape_attributes': {'name': 'rect', 'x': 20, 'y': 20, 'width': 3017, 'height': 4560}, 'region_attributes': {'label': 'sugarcane'}}], 'file_attributes': {}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1843, 0.1882, 0.1961,  ..., 0.3725, 0.3608, 0.3569],\n",
       "          [0.1843, 0.1922, 0.1961,  ..., 0.3451, 0.3373, 0.3490],\n",
       "          [0.1843, 0.1961, 0.2000,  ..., 0.3333, 0.3373, 0.3608],\n",
       "          ...,\n",
       "          [0.3608, 0.3725, 0.3961,  ..., 0.2902, 0.3294, 0.3569],\n",
       "          [0.2824, 0.3059, 0.3294,  ..., 0.3569, 0.3725, 0.3569],\n",
       "          [0.2039, 0.2314, 0.2471,  ..., 0.3882, 0.3686, 0.3529]],\n",
       " \n",
       "         [[0.1725, 0.1765, 0.1843,  ..., 0.5137, 0.4941, 0.4745],\n",
       "          [0.1725, 0.1804, 0.1843,  ..., 0.4784, 0.4549, 0.4588],\n",
       "          [0.1725, 0.1843, 0.1882,  ..., 0.4431, 0.4353, 0.4471],\n",
       "          ...,\n",
       "          [0.3216, 0.3333, 0.3608,  ..., 0.3608, 0.4039, 0.4314],\n",
       "          [0.2549, 0.2784, 0.3020,  ..., 0.4314, 0.4510, 0.4353],\n",
       "          [0.1882, 0.2157, 0.2314,  ..., 0.4706, 0.4588, 0.4431]],\n",
       " \n",
       "         [[0.1451, 0.1490, 0.1569,  ..., 0.1294, 0.1255, 0.1216],\n",
       "          [0.1451, 0.1529, 0.1569,  ..., 0.1098, 0.1098, 0.1255],\n",
       "          [0.1451, 0.1569, 0.1608,  ..., 0.1098, 0.1176, 0.1529],\n",
       "          ...,\n",
       "          [0.2745, 0.2863, 0.3020,  ..., 0.0235, 0.0549, 0.0824],\n",
       "          [0.2157, 0.2392, 0.2627,  ..., 0.0824, 0.1020, 0.0863],\n",
       "          [0.1529, 0.1804, 0.1961,  ..., 0.1137, 0.1059, 0.0902]]]),\n",
       " {'boxes': tensor([[ 887, 3249, 2464, 4601],\n",
       "          [   7,    0, 3065, 1522],\n",
       "          [ 608, 1563, 3052, 3236],\n",
       "          [2499, 3639, 3059, 4602],\n",
       "          [  20,   20, 3037, 4580]]),\n",
       "  'labels': tensor([1, 1, 1, 1, 1]),\n",
       "  'img_id': tensor([302])},\n",
       " 'i3092.jpg')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.__getitem__(302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
