{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dependencies; setup logger\n",
    "import torch, torchvision\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import DatasetMapper   # the default mapper\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader\n",
    "from fvcore.transforms.transform import NoOpTransform\n",
    "\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "import detectron2.utils.comm as comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weed_dicts(img_dir, file_list):\n",
    "    json_file = os.path.join(img_dir, \"_weed_labels.json\")\n",
    "    with open(json_file) as f:\n",
    "        img_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(img_anns.values()):\n",
    "        if v[\"filename\"] in file_list:  # train or test data\n",
    "            record = {}\n",
    "            filename = os.path.join(img_dir, v[\"filename\"])\n",
    "            # print(filename)\n",
    "            height, width = cv2.imread(filename).shape[:2]\n",
    "\n",
    "            record[\"file_name\"] = filename\n",
    "            record[\"image_id\"] = idx\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "\n",
    "            annos = v[\"regions\"]   # List of object attributes\n",
    "            objs = []\n",
    "            for anno in annos:\n",
    "                if anno[\"region_attributes\"][\"label\"] == \"weed\":\n",
    "                    sa = anno[\"shape_attributes\"]\n",
    "                    obj = {\n",
    "                        \"bbox\": [sa['x'], sa['y'], sa['width'], sa['height']],\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS, # or XYXY_ABS\n",
    "                        \"category_id\": 0 if anno[\"region_attributes\"][\"label\"] == \"weed\" else 1\n",
    "                    }\n",
    "                    objs.append(obj)\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and register datasets\n",
    "img_dir = '/home/mschoder/data/allweeds_600x400/'\n",
    "\n",
    "json_file = os.path.join(img_dir, \"_weed_labels.json\")\n",
    "print(json_file)\n",
    "with open(json_file) as f:\n",
    "    img_anns = json.load(f)\n",
    "\n",
    "file_list = sorted([k for k in img_anns.keys()])\n",
    "print(\"Number of images: \", len(file_list))\n",
    "np.random.seed(31)\n",
    "np.random.shuffle(file_list)\n",
    "val_pct = 0.2  # 60-20-20 split\n",
    "file_lists = {\n",
    "    \"val\": file_list[:int(val_pct * len(file_list))],\n",
    "    \"test\": file_list[int(val_pct * len(file_list)):int(2*val_pct * len(file_list))],\n",
    "    \"train\": file_list[int(2*val_pct * len(file_list)):]\n",
    "}\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "MetadataCatalog.clear()\n",
    "for d in [\"train\", \"val\", \"test\"]:\n",
    "    DatasetCatalog.register(\"weeds_\" + d, lambda d=d: get_weed_dicts(img_dir, file_lists[d]))\n",
    "    MetadataCatalog.get(\"weeds_\" + d).set(thing_classes=[\"weed\"])\n",
    "weeds_metadata = MetadataCatalog.get(\"weeds_train\")\n",
    "MetadataCatalog.get(\"weeds_val\").set(json_file='./coco_eval/weeds_val_coco_format.json')\n",
    "\n",
    "# Build full dict\n",
    "print('Creating datasets...')\n",
    "# print(file_lists)\n",
    "dataset_dicts = get_weed_dicts(img_dir, file_lists['train'])\n",
    "print('Datasets created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetadataCatalog.get(\"weeds_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define transformations\n",
    "class HSV_EQ_Augmentation(T.Augmentation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_transform(self, image):\n",
    "        return HSV_EQ_Transform()\n",
    "\n",
    "class HSV_EQ_Transform(T.Transform):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._set_attributes(locals())\n",
    "\n",
    "    def apply_image(self,img):\n",
    "        clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(8,8))\n",
    "        img_HSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        h,s,v = cv2.split(img_HSV)\n",
    "        cl2 = clahe.apply(v)\n",
    "        img_HSV_new = cv2.merge((h,s,cl2))\n",
    "        img_BGR = cv2.cvtColor(img_HSV_new,cv2.COLOR_HSV2BGR)\n",
    "        return img_BGR\n",
    "\n",
    "    def apply_coords(self, coords):\n",
    "        return coords\n",
    "\n",
    "    def inverse(self):\n",
    "        return NoOpTransform()\n",
    "\n",
    "    def apply_segmentation(self, segmentation):\n",
    "        return segmentation\n",
    "\n",
    "class HLS_EQ_Augmentation(T.Augmentation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_transform(self, image):\n",
    "        return HLS_EQ_Transform()\n",
    "\n",
    "class HLS_EQ_Transform(T.Transform):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._set_attributes(locals())\n",
    "\n",
    "    def apply_image(self,img):\n",
    "        clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(8,8))\n",
    "        img_HLS = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "        h,l,s = cv2.split(img_HLS)\n",
    "        cl2 = clahe.apply(l)\n",
    "        img_HLS_new = cv2.merge((h,cl2,s))\n",
    "        img_BGR = cv2.cvtColor(img_HLS_new,cv2.COLOR_HLS2BGR)\n",
    "        return img_BGR\n",
    "\n",
    "    def apply_coords(self, coords):\n",
    "        return coords\n",
    "\n",
    "    def inverse(self):\n",
    "        return NoOpTransform()\n",
    "\n",
    "    def apply_segmentation(self, segmentation):\n",
    "        return segmentation\n",
    "\n",
    "class NDI_CIVE_ExG_Augmentation(T.Augmentation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_transform(self, image):\n",
    "        return NDI_CIVE_ExG_Transform()\n",
    "\n",
    "class NDI_CIVE_ExG_Transform(T.Transform):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._set_attributes(locals())\n",
    "\n",
    "    def apply_image(self,img):\n",
    "        B,G,R = cv2.split(img)\n",
    "        #NDI\n",
    "        NDI=128*((G-R)/(G+R)+1)\n",
    "        #CIVE\n",
    "        CIVE = 0.441*R-.881*G+.385*B+18.78745\n",
    "        #ExG\n",
    "        R_st = R/255; G_st = G/255 ; B_st = B/255\n",
    "        tot =  R_st+G_st+B_st\n",
    "        r = R_st/tot; g = G_st/tot; b = B_st/tot\n",
    "        ExG = 2*g-r-b\n",
    "        img_out = cv2.merge((NDI,CIVE,ExG))\n",
    "        return img_out\n",
    "\n",
    "    def apply_coords(self, coords):\n",
    "        return coords\n",
    "\n",
    "    def inverse(self):\n",
    "        return NoOpTransform()\n",
    "\n",
    "    def apply_segmentation(self, segmentation):\n",
    "        return segmentation\n",
    "\n",
    "class ExG_Augmentation(T.Augmentation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_transform(self, image):\n",
    "        return ExG_Transform()\n",
    "\n",
    "class ExG_Transform(T.Transform):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._set_attributes(locals())\n",
    "\n",
    "    def apply_image(self,img):\n",
    "        B,G,R = cv2.split(img)\n",
    "        #ExG\n",
    "        R_st = R/255; G_st = G/255 ; B_st = B/255\n",
    "        tot =  R_st+G_st+B_st\n",
    "        r = R_st/tot; g = G_st/tot; b = B_st/tot\n",
    "        ExG = 2*g-r-b\n",
    "        return ExG\n",
    "\n",
    "    def apply_coords(self, coords):\n",
    "        return coords\n",
    "\n",
    "    def inverse(self):\n",
    "        return NoOpTransform()\n",
    "\n",
    "    def apply_segmentation(self, segmentation):\n",
    "        return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mapper_train(dataset_dict):\n",
    "    # Implement a mapper, similar to the default DatasetMapper, but with your own customizations\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"RGB\")\n",
    "    transform_list = []\n",
    "    # Rotate 90 degrees to landscape if image is in portrait format\n",
    "    height, width, _ = image.shape\n",
    "    if height > width:\n",
    "        transform_list.append(T.RotationTransform)\n",
    "    transform_list = [T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "                      NoOpTransform(),\n",
    "                      ]\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict\n",
    "\n",
    "def custom_mapper_test(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict) \n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"RGB\")\n",
    "    transform_list = []\n",
    "    height, width, _ = image.shape\n",
    "    if height > width:\n",
    "        transform_list.append(T.RotationTransform)\n",
    "    transform_list = [NoOpTransform(),\n",
    "                      ]\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict\n",
    "\n",
    "## Define hook for validation loss\n",
    "class ValidationLoss(HookBase):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg.clone()\n",
    "        self.cfg.DATASETS.TRAIN = cfg.DATASETS.TEST\n",
    "        self._loader = iter(build_detection_train_loader(self.cfg, mapper=custom_mapper_test)) \n",
    "        \n",
    "    def after_step(self):\n",
    "        data = next(self._loader)\n",
    "        with torch.no_grad():\n",
    "            loss_dict = self.trainer.model(data)\n",
    "            \n",
    "            losses = sum(loss_dict.values())\n",
    "            assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "            loss_dict_reduced = {\"val_\" + k: v.item() for k, v in \n",
    "                                 comm.reduce_dict(loss_dict).items()}\n",
    "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "            if comm.is_main_process():\n",
    "                self.trainer.storage.put_scalars(total_val_loss=losses_reduced, \n",
    "                                                 **loss_dict_reduced)\n",
    "\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "            output_folder = \"coco_eval\"\n",
    "\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=custom_mapper_train)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build_detection_test_loader(cfg, cfg.DATASETS.TEST[0], mapper=custom_mapper_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Trained Model and Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_models = [\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\",\n",
    "                     \"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\",\n",
    "                     \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"]\n",
    "\n",
    "color_tfs = [NoOpTransform(), HSV_EQ_Transform(), \n",
    "             HLS_EQ_Transform(), NDI_CIVE_ExG_Transform()]\n",
    "\n",
    "model_dir = '/home/mschoder/experiment_outputs/model_COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml_lr_0.00025_iters_750_tf_NoOpTransform()/'\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(pretrained_models[0]))\n",
    "cfg.MODEL.WEIGHTS = os.path.join(model_dir, \"model_final.pth\")  # path to trained model\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only one class (weed), other class is background\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"weeds_train\",)\n",
    "cfg.DATASETS.TEST = (\"weeds_val\",)\n",
    "\n",
    "cfg.OUTPUT_DIR = model_dir\n",
    "\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "trainer = CocoTrainer(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(12, 10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .3, wspace=.3)\n",
    "fig.tight_layout()\n",
    "axs = axs.ravel()\n",
    "\n",
    "val_dict = get_weed_dicts(img_dir, file_lists['val'])\n",
    "for i,d in enumerate(random.sample(val_dict, 4)):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=weeds_metadata, \n",
    "                   scale=1)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    axs[i].imshow(out.get_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_vals = np.arange(.1, 1, 0.1).tolist()\n",
    "\n",
    "\n",
    "# for thresh in iou_vals:\n",
    "#     cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = thresh\n",
    "#     evaluator = COCOEvaluator(\"weeds_val\", cfg, False, output_dir=model_dir)\n",
    "#     val_loader = build_detection_test_loader(cfg, \"weeds_val\")\n",
    "#     print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
    "#     # another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "evaluator = COCOEvaluator(\"weeds_val\", cfg, False, output_dir=False)\n",
    "# val_loader = build_detection_test_loader(cfg, \"weeds_val\")\n",
    "val_loader = CocoTrainer.build_test_loader(cfg, \"weeds_val\")\n",
    "res = inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
